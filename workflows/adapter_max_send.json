{
  "name": "adapter_max_send",
  "nodes": [
    {
      "parameters": {
        "inputSource": "passthrough"
      },
      "id": "8740dc61-f308-4328-9152-2e4b886abafc",
      "name": "When Executed by Another Workflow",
      "type": "n8n-nodes-base.executeWorkflowTrigger",
      "typeVersion": 1.1,
      "position": [
        720,
        128
      ]
    },
    {
      "parameters": {
        "httpMethod": "POST",
        "path": "autoposting-adapter-max-test",
        "responseMode": "lastNode",
        "options": {}
      },
      "id": "afd593f0-7a8e-486c-8b97-a661a227beb8",
      "name": "Adapter MAX Smoke Webhook",
      "type": "n8n-nodes-base.webhook",
      "typeVersion": 2.1,
      "position": [
        720,
        352
      ],
      "webhookId": "a8ca47e0-4b6c-4117-97af-728a65b88a42"
    },
    {
      "parameters": {
        "jsCode": "const src = $input.first()?.json?.body ?? $input.first()?.json ?? {};\nconst delivery = src.delivery ?? {};\nconst channelInput = src.channel ?? {};\nconst payload = delivery.payload ?? {};\nconst sourceMedia = Array.isArray(payload.media) ? payload.media : [];\n\nconst renderedText = String(\n  delivery.rendered_text ?? src.rendered_text ?? payload.text ?? '',\n);\n\nconst parseModeRaw = String(\n  delivery.meta?.parse_mode ?? src.meta?.parse_mode ?? 'None',\n);\nlet maxFormat = null;\nif (parseModeRaw === 'HTML') maxFormat = 'html';\nif (parseModeRaw === 'Markdown' || parseModeRaw === 'MarkdownV2') maxFormat = 'markdown';\n\nconst env = typeof process !== 'undefined' && process.env ? process.env : {};\n\nconst channelId =\n  channelInput.channel_id ?? delivery.channel_id ?? src.channel_id ?? null;\nconst authRef = channelInput.auth_ref ?? src.auth_ref ?? null;\n\nconst channel = {\n  ...channelInput,\n  channel_id: channelId,\n  auth_ref: authRef,\n};\n\nconst authHeaderName = String(\n  channel.auth_header_name ?? env.MAX_AUTH_HEADER ?? 'Authorization',\n);\nconst authHeaderValue = String(\n  channel.max_auth ?? channel.auth_token ?? channel.token ?? env.MAX_AUTH_VALUE ?? '',\n).trim();\n\nlet baseUrl = String(\n  channel.max_base_url ??\n    channel.base_url ??\n    env.MAX_API_BASE_URL ??\n    'https://platform-api.max.ru',\n);\nwhile (baseUrl.endsWith('/')) baseUrl = baseUrl.slice(0, -1);\n\nconst parseObject = (value) => {\n  if (value && typeof value === 'object' && !Array.isArray(value)) return value;\n  if (typeof value !== 'string') return {};\n  try {\n    const parsed = JSON.parse(value);\n    return parsed && typeof parsed === 'object' && !Array.isArray(parsed) ? parsed : {};\n  } catch {\n    return {};\n  }\n};\n\nconst renderMeta = parseObject(\n  delivery.render_meta ?? src.render_meta ?? payload.render_meta ?? {},\n);\n\nconst rawSeriesProgressCandidates = [\n  renderMeta.series_progress,\n  delivery.series_progress,\n  src.series_progress,\n  payload.series_progress,\n  delivery.meta?.series_progress,\n  src.meta?.series_progress,\n]\n  .map(parseObject)\n  .filter((value) => Object.keys(value).length > 0);\n\nconst toIndex = (value) => {\n  if (value === null || value === undefined || value === '') return null;\n  const num = Number(value);\n  if (!Number.isFinite(num)) return null;\n  if (!Number.isInteger(num) || num < 0) return null;\n  return num;\n};\n\nconst toNonEmptyString = (value) => {\n  if (value === null || value === undefined) return null;\n  const text = String(value).trim();\n  return text ? text : null;\n};\n\nconst ackedSet = new Set();\nconst providerMessageIds = [];\nconst providerMessageIdSet = new Set();\n\nconst pushProviderMessageId = (value) => {\n  const text = toNonEmptyString(value);\n  if (!text || providerMessageIdSet.has(text)) return;\n  providerMessageIdSet.add(text);\n  providerMessageIds.push(text);\n};\n\nfor (const progress of rawSeriesProgressCandidates) {\n  const ackedIndexes = Array.isArray(progress.acked_indexes)\n    ? progress.acked_indexes\n    : [];\n  for (const value of ackedIndexes) {\n    const idx = toIndex(value);\n    if (idx !== null) ackedSet.add(idx);\n  }\n\n  const sentIndexes = Array.isArray(progress.sent_indexes)\n    ? progress.sent_indexes\n    : [];\n  for (const value of sentIndexes) {\n    const idx = toIndex(value);\n    if (idx !== null) ackedSet.add(idx);\n  }\n\n  const sentItems = Array.isArray(progress.sent_items) ? progress.sent_items : [];\n  for (const item of sentItems) {\n    if (item && typeof item === 'object') {\n      const idx = toIndex(item.index ?? item.media_index ?? item.item_index);\n      if (idx !== null) ackedSet.add(idx);\n      pushProviderMessageId(item.provider_message_id ?? item.message_id ?? item.id);\n      continue;\n    }\n\n    const idx = toIndex(item);\n    if (idx !== null) ackedSet.add(idx);\n  }\n\n  const progressMessageIds = Array.isArray(progress.provider_message_ids)\n    ? progress.provider_message_ids\n    : [];\n  for (const value of progressMessageIds) {\n    if (value && typeof value === 'object') {\n      pushProviderMessageId(\n        value.provider_message_id ?? value.message_id ?? value.id,\n      );\n    } else {\n      pushProviderMessageId(value);\n    }\n  }\n}\n\nconst seriesProgress = {\n  acked_indexes: Array.from(ackedSet).sort((a, b) => a - b),\n  provider_message_ids: providerMessageIds,\n};\n\nconst normalizedMedia = sourceMedia\n  .map((itemRaw, index) => {\n    const item = itemRaw && typeof itemRaw === 'object' ? itemRaw : null;\n    if (!item) return null;\n\n    return {\n      index,\n      blob_ref: item.blob_ref ?? null,\n      type: item.type ?? null,\n      kind: item.kind ?? null,\n      origin_url: item.origin_url ?? null,\n      url: item.url ?? null,\n      media_url: item.media_url ?? null,\n      upload_id: item.upload_id ?? null,\n      upload_token: item.upload_token ?? null,\n      attachment_id: item.attachment_id ?? null,\n      file_id: item.file_id ?? null,\n      token: item.token ?? null,\n      max_upload_id: item.max_upload_id ?? null,\n    };\n  })\n  .filter(Boolean);\n\nconst mediaBlobRefs = normalizedMedia\n  .map((item) => (typeof item?.blob_ref === 'string' ? item.blob_ref.trim() : ''))\n  .filter((blobRef) => blobRef.length > 0);\n\nreturn [\n  {\n    json: {\n      workspace_id: src.workspace_id ?? null,\n      delivery,\n      channel,\n      channel_id: channelId,\n      auth_ref: authRef,\n      target_id: channel.target_id ?? delivery.target_id ?? src.target_id ?? null,\n      rendered_text: renderedText,\n      media: normalizedMedia,\n      media_blob_refs: mediaBlobRefs,\n      series_progress: seriesProgress,\n      max_format: maxFormat,\n      max_base_url: baseUrl,\n      auth_header_name: authHeaderName,\n      auth_header_value: authHeaderValue,\n    },\n  },\n];"
      },
      "id": "ae5bbd82-9521-4fd3-a38a-8ad0bee6c652",
      "name": "Normalize Adapter Input",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        960,
        240
      ]
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "version": 3,
            "leftValue": "",
            "caseSensitive": true,
            "typeValidation": "strict"
          },
          "conditions": [
            {
              "id": "max-cond-can-call",
              "leftValue": "={{ !!$json.target_id && !!$json.auth_header_value }}",
              "rightValue": "",
              "operator": {
                "type": "boolean",
                "operation": "true",
                "singleValue": true
              }
            }
          ],
          "combinator": "and"
        },
        "options": {}
      },
      "id": "1295cc2d-8640-46f3-9aa9-e07b4ff9dc64",
      "name": "Can Call MAX API?",
      "type": "n8n-nodes-base.if",
      "typeVersion": 2.3,
      "position": [
        1168,
        240
      ]
    },
    {
      "parameters": {
        "jsCode": "const input = $input.first()?.json ?? {};\nconst env = typeof process !== 'undefined' && process.env ? process.env : {};\nconst simulateEnabledByEnv = String(env.ADAPTER_MAX_SIMULATE ?? '').toLowerCase() === 'true';\n\nconst simulateOverride = input?.channel?.simulate;\nconst hasOverride =\n  typeof simulateOverride === 'boolean' ||\n  typeof simulateOverride === 'number' ||\n  typeof simulateOverride === 'string';\n\nconst simulateEnabled = hasOverride\n  ? String(simulateOverride).toLowerCase() === 'true'\n  : simulateEnabledByEnv;\n\nif (simulateEnabled) {\n  return [\n    {\n      json: {\n        ok: true,\n        provider_message_id: `max-mock-${Date.now()}`,\n        raw: {\n          simulated: true,\n          reason: hasOverride\n            ? `channel.simulate=${String(simulateOverride)}`\n            : 'ADAPTER_MAX_SIMULATE=true',\n          target_id: input.target_id ?? null,\n        },\n      },\n    },\n  ];\n}\n\nreturn [\n  {\n    json: {\n      ok: false,\n      error: {\n        category: 'PERMANENT',\n        scope: 'delivery',\n        code: 'missing_adapter_inputs',\n        message:\n          'adapter_max_send requires target_id and auth header value (set ADAPTER_MAX_SIMULATE=true or channel.simulate=true to force simulation)',\n        raw: {\n          has_target_id: !!input.target_id,\n          has_auth: !!input.auth_header_value,\n          simulate_enabled_by_env: simulateEnabledByEnv,\n          simulate_override: hasOverride ? String(simulateOverride) : null,\n        },\n      },\n    },\n  },\n];"
      },
      "id": "b6d0f1f4-f2c3-4e8d-88ec-da0e8af77e94",
      "name": "Simulate MAX Success",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1408,
        416
      ]
    },
    {
      "parameters": {
        "jsCode": "const input = $input.first()?.json ?? {};\n\nconst env = typeof process !== 'undefined' && process.env ? process.env : {};\nconst nodeVersion = typeof process !== 'undefined' ? process.version : null;\n\nconst textForValidation = String(input.rendered_text ?? '');\nconst allMediaForValidation = Array.isArray(input.media) ? input.media : [];\n\nconst toIndex = (value) => {\n  if (value === null || value === undefined || value === '') return null;\n  const num = Number(value);\n  if (!Number.isFinite(num)) return null;\n  if (!Number.isInteger(num) || num < 0) return null;\n  return num;\n};\n\nconst toNonEmptyString = (value) => {\n  if (value === null || value === undefined) return null;\n  const text = String(value).trim();\n  return text ? text : null;\n};\n\nconst getAckedIndexes = (seriesProgress) => {\n  if (!seriesProgress || typeof seriesProgress !== 'object') return [];\n  const values = [\n    ...(Array.isArray(seriesProgress.acked_indexes) ? seriesProgress.acked_indexes : []),\n    ...(Array.isArray(seriesProgress.sent_indexes) ? seriesProgress.sent_indexes : []),\n  ];\n\n  const sentItems = Array.isArray(seriesProgress.sent_items) ? seriesProgress.sent_items : [];\n  for (const item of sentItems) {\n    if (item && typeof item === 'object') {\n      values.push(item.index ?? item.media_index ?? item.item_index);\n      continue;\n    }\n    values.push(item);\n  }\n\n  const set = new Set();\n  for (const value of values) {\n    const idx = toIndex(value);\n    if (idx !== null) set.add(idx);\n  }\n\n  return Array.from(set).sort((a, b) => a - b);\n};\n\nconst getProgressProviderMessageIds = (seriesProgress) => {\n  if (!seriesProgress || typeof seriesProgress !== 'object') return [];\n\n  const values = [];\n\n  const direct = Array.isArray(seriesProgress.provider_message_ids)\n    ? seriesProgress.provider_message_ids\n    : [];\n  for (const value of direct) {\n    if (value && typeof value === 'object') {\n      values.push(value.provider_message_id ?? value.message_id ?? value.id ?? null);\n    } else {\n      values.push(value);\n    }\n  }\n\n  const sentItems = Array.isArray(seriesProgress.sent_items) ? seriesProgress.sent_items : [];\n  for (const item of sentItems) {\n    if (!item || typeof item !== 'object') continue;\n    values.push(item.provider_message_id ?? item.message_id ?? item.id ?? null);\n  }\n\n  const uniq = [];\n  const seen = new Set();\n  for (const value of values) {\n    const text = toNonEmptyString(value);\n    if (!text || seen.has(text)) continue;\n    seen.add(text);\n    uniq.push(text);\n  }\n\n  return uniq;\n};\n\nconst incomingSeriesProgress =\n  input.series_progress && typeof input.series_progress === 'object'\n    ? input.series_progress\n    : {};\n\nconst alreadyAckedIndexes = getAckedIndexes(incomingSeriesProgress);\nconst alreadyAckedSet = new Set(alreadyAckedIndexes);\n\nconst mediaQueue = allMediaForValidation\n  .map((item, index) => ({ item, original_index: index }))\n  .filter(({ original_index }) => !alreadyAckedSet.has(original_index));\n\nconst maxTextLimitForValidation = Number(env.ADAPTER_MAX_TEXT_LIMIT || 4000);\nif (Number.isFinite(maxTextLimitForValidation) && textForValidation.length > maxTextLimitForValidation) {\n  return [\n    {\n      json: {\n        ok: false,\n        error: {\n          category: 'PERMANENT',\n          scope: 'delivery',\n          code: 'message_too_long',\n          message: `MAX message too long: len=${textForValidation.length} > limit=${maxTextLimitForValidation}`,\n          raw: { len: textForValidation.length, limit: maxTextLimitForValidation },\n        },\n      },\n    },\n  ];\n}\n\nfor (const entry of mediaQueue) {\n  const item = entry.item;\n  const originalIndex = entry.original_index;\n\n  if (!item || typeof item !== 'object') continue;\n\n  const existingToken =\n    item.upload_id ??\n    item.upload_token ??\n    item.max_upload_id ??\n    item.token ??\n    (item.attachment_id ? null : item.file_id) ??\n    null;\n  if (existingToken) continue;\n\n  const originUrl = item.origin_url ?? item.url ?? item.media_url ?? null;\n  if (originUrl) continue;\n\n  return [\n    {\n      json: {\n        ok: false,\n        error: {\n          category: 'PERMANENT',\n          scope: 'delivery',\n          code: 'media_missing_origin',\n          message: `media[${originalIndex}] missing origin_url/url/media_url`,\n          raw: { index: originalIndex, item },\n        },\n        series_progress_delta: { acked_indexes: [] },\n        raw: { series_results: [] },\n      },\n    },\n  ];\n}\n\nconst fetchFn = globalThis.fetch;\nif (typeof fetchFn !== 'function') {\n  return [\n    {\n      json: {\n        ok: false,\n        error: {\n          category: 'TRANSIENT',\n          scope: 'platform',\n          code: 'fetch_unavailable',\n          message: 'global fetch is not available in n8n Code node runtime',\n          raw: { node_version: nodeVersion },\n        },\n      },\n    },\n  ];\n}\n\nlet baseUrl = String(input.max_base_url || '');\nwhile (baseUrl.endsWith('/')) baseUrl = baseUrl.slice(0, -1);\n\nconst workspaceId =\n  input.workspace_id === null || input.workspace_id === undefined\n    ? null\n    : String(input.workspace_id);\nconst targetId = input.target_id;\nconst text = String(input.rendered_text ?? '');\nconst format = input.max_format || null;\n\nconst authHeaderName = String(input.auth_header_name || 'Authorization');\nconst authHeaderValue = String(input.auth_header_value || '').trim();\n\nconst cacheUpdatesByBlobRef = {};\nconst listCacheUpdates = () => Object.values(cacheUpdatesByBlobRef);\n\nconst normalizeExpiresAt = (value) => {\n  if (value === null || value === undefined) return null;\n  const textValue = String(value).trim();\n  if (!textValue) return null;\n  const timestamp = Date.parse(textValue);\n  return Number.isFinite(timestamp) ? new Date(timestamp).toISOString() : null;\n};\n\nconst appendCacheUpdate = ({ blobRef, fileId, mediaIndex, source, expiresAt }) => {\n  const normalizedBlobRef = typeof blobRef === 'string' ? blobRef.trim() : '';\n  const normalizedFileId =\n    fileId === null || fileId === undefined ? '' : String(fileId).trim();\n\n  if (!workspaceId || !normalizedBlobRef || !normalizedFileId) {\n    return;\n  }\n\n  cacheUpdatesByBlobRef[normalizedBlobRef] = {\n    workspace_id: workspaceId,\n    blob_ref: normalizedBlobRef,\n    file_id: normalizedFileId,\n    expires_at: normalizeExpiresAt(expiresAt),\n    meta: {\n      source,\n      media_index: Number.isFinite(Number(mediaIndex)) ? Number(mediaIndex) : null,\n    },\n  };\n};\n\nconst withCacheUpdates = (payload) => ({\n  ...payload,\n  cache_updates: listCacheUpdates(),\n});\n\nconst wrapOk = (payload) => [{ json: withCacheUpdates(payload) }];\nconst wrapErr = (payload) => [{ json: withCacheUpdates(payload) }];\n\nconst maxTextLimit = Number(env.ADAPTER_MAX_TEXT_LIMIT || 4000);\nif (Number.isFinite(maxTextLimit) && text.length > maxTextLimit) {\n  return wrapErr({\n    ok: false,\n    error: {\n      category: 'PERMANENT',\n      scope: 'delivery',\n      code: 'message_too_long',\n      message: `MAX message too long: len=${text.length} > limit=${maxTextLimit}`,\n      raw: { len: text.length, limit: maxTextLimit },\n    },\n  });\n}\n\nif (!baseUrl || !targetId || !authHeaderValue) {\n  return wrapErr({\n    ok: false,\n    error: {\n      category: 'PERMANENT',\n      scope: 'delivery',\n      code: 'missing_adapter_inputs',\n      message:\n        'adapter_max_send requires target_id, MAX_API_BASE_URL, and auth header value',\n      raw: {\n        has_base_url: !!baseUrl,\n        has_target_id: !!targetId,\n        has_auth: !!authHeaderValue,\n      },\n    },\n  });\n}\n\nconst headersAuth = { [authHeaderName]: authHeaderValue };\n\nconst timeoutMsMessage = Number(env.MAX_MESSAGE_TIMEOUT_SEC || 30) * 1000;\nconst timeoutMsUpload = Number(env.MAX_UPLOAD_TIMEOUT_SEC || 60) * 1000;\nconst timeoutMsDownload = Number(env.DOWNLOAD_TIMEOUT_MS || 30000);\n\nconst fetchWithTimeout = async (url, opts, timeoutMs) => {\n  const controller = new AbortController();\n  const timeout = setTimeout(() => controller.abort(), timeoutMs);\n  try {\n    return await fetchFn(url, { ...opts, signal: controller.signal });\n  } finally {\n    clearTimeout(timeout);\n  }\n};\n\nconst safeJsonParse = (textValue) => {\n  if (!textValue) return { value: null };\n  try {\n    return { value: JSON.parse(textValue) };\n  } catch {\n    return { value: { _raw: textValue } };\n  }\n};\n\nconst extractMessage = (body, fallback) => {\n  if (typeof body === 'string') return { value: body };\n  const msg =\n    body?.message ??\n    body?.description ??\n    body?.detail ??\n    body?.error?.message ??\n    body?.error?.description ??\n    body?.error ??\n    body?.reason;\n  return { value: String(msg ?? fallback ?? 'max_request_failed') };\n};\n\nconst parseRetryAfterMs = (respHeaders, body, message) => {\n  const headerValue = respHeaders?.get?.('retry-after');\n  if (headerValue) {\n    const sec = Number(headerValue);\n    if (Number.isFinite(sec) && sec > 0) return { value: sec * 1000 };\n  }\n\n  const retrySec = Number(\n    body?.retry_after ?? body?.retryAfter ?? body?.error?.retry_after,\n  );\n  if (Number.isFinite(retrySec) && retrySec > 0) return { value: retrySec * 1000 };\n\n  const match = String(message || '').match(/retry after\\s*(\\d+)/i);\n  if (match) {\n    const sec = Number(match[1]);\n    if (Number.isFinite(sec) && sec > 0) return { value: sec * 1000 };\n  }\n\n  return { value: null };\n};\n\nconst classifyError = ({ status, body, message, respHeaders, stage, raw }) => {\n  const msg = String(message ?? 'max_send_failed');\n  const statusNum = Number(status ?? 0);\n\n  let category = 'PERMANENT';\n  let scope = 'delivery';\n  let code = 'max_bad_request';\n\n  if (/attachment\\.not\\.ready/i.test(msg)) {\n    category = 'TRANSIENT';\n    scope = 'delivery';\n    code = 'attachment_not_ready';\n  } else if (\n    statusNum === 429 ||\n    /rate limit|too many requests|retry after/i.test(msg)\n  ) {\n    category = 'TRANSIENT';\n    scope = 'platform';\n    code = 'rate_limited';\n  } else if (\n    statusNum >= 500 ||\n    /timeout|timed out|network|socket|econn|etimedout/i.test(msg)\n  ) {\n    category = 'TRANSIENT';\n    scope = 'platform';\n    code = 'upstream_unavailable';\n  } else if (\n    statusNum === 401 ||\n    statusNum === 403 ||\n    /unauthorized|forbidden|access denied/i.test(msg)\n  ) {\n    category = 'PERMANENT';\n    scope = 'channel';\n    code = 'channel_unavailable';\n  }\n\n  const retryAfterWrap = parseRetryAfterMs(respHeaders, body, msg);\n  const retryAfterMs = retryAfterWrap.value;\n\n  return {\n    category,\n    scope,\n    code,\n    retry_after_ms: Number.isFinite(retryAfterMs) ? retryAfterMs : undefined,\n    message: msg,\n    raw: {\n      stage,\n      status: statusNum || undefined,\n      body,\n      raw,\n    },\n  };\n};\n\nconst requestJson = async ({ url, method, body, timeoutMs, stage }) => {\n  const headers = {\n    ...headersAuth,\n    'Content-Type': 'application/json',\n  };\n\n  let resp;\n  try {\n    resp = await fetchWithTimeout(\n      url,\n      {\n        method,\n        headers,\n        body: body === undefined ? undefined : JSON.stringify(body),\n      },\n      timeoutMs,\n    );\n  } catch (e) {\n    const message = e?.name === 'AbortError' ? 'timeout' : String(e?.message ?? e);\n    return {\n      ok: false,\n      status: 0,\n      body: null,\n      message,\n      error: classifyError({\n        status: 0,\n        body: null,\n        message,\n        respHeaders: null,\n        stage,\n        raw: { kind: 'fetch_error' },\n      }),\n    };\n  }\n\n  const textValue = await resp.text().catch(() => '');\n  const parsedWrap = safeJsonParse(textValue);\n  const parsed = parsedWrap.value;\n\n  if (!resp.ok) {\n    const messageWrap = extractMessage(parsed ?? textValue, `http_${resp.status}`);\n    const message = messageWrap.value;\n    return {\n      ok: false,\n      status: resp.status,\n      body: parsed ?? textValue,\n      message,\n      error: classifyError({\n        status: resp.status,\n        body: parsed ?? textValue,\n        message,\n        respHeaders: resp.headers,\n        stage,\n        raw: null,\n      }),\n    };\n  }\n\n  return {\n    ok: true,\n    status: resp.status,\n    body: parsed,\n    message: null,\n    error: null,\n  };\n};\n\nconst requestBinary = async ({ url, timeoutMs, stage }) => {\n  let resp;\n  try {\n    resp = await fetchWithTimeout(url, { method: 'GET' }, timeoutMs);\n  } catch (e) {\n    const message = e?.name === 'AbortError' ? 'timeout' : String(e?.message ?? e);\n    return {\n      ok: false,\n      status: 0,\n      buffer: null,\n      contentType: null,\n      error: classifyError({\n        status: 0,\n        body: null,\n        message,\n        respHeaders: null,\n        stage,\n        raw: { kind: 'fetch_binary_error' },\n      }),\n    };\n  }\n\n  if (!resp.ok) {\n    const textValue = await resp.text().catch(() => '');\n    const parsedWrap = safeJsonParse(textValue);\n    const parsed = parsedWrap.value;\n    const messageWrap = extractMessage(parsed ?? textValue, `http_${resp.status}`);\n    const message = messageWrap.value;\n\n    return {\n      ok: false,\n      status: resp.status,\n      buffer: null,\n      contentType: resp.headers?.get?.('content-type') ?? null,\n      error: classifyError({\n        status: resp.status,\n        body: parsed ?? textValue,\n        message,\n        respHeaders: resp.headers,\n        stage,\n        raw: null,\n      }),\n    };\n  }\n\n  const arrayBuffer = await resp.arrayBuffer();\n  const buffer = Buffer.from(arrayBuffer);\n\n  return {\n    ok: true,\n    status: resp.status,\n    buffer,\n    contentType: resp.headers?.get?.('content-type') ?? null,\n    error: null,\n  };\n};\n\nconst uploadMultipart = async ({ url, buffer, contentType, timeoutMs, stage }) => {\n  if (typeof FormData !== 'function' || typeof Blob !== 'function') {\n    return {\n      ok: false,\n      status: 0,\n      error: classifyError({\n        status: 0,\n        body: null,\n        message: 'FormData/Blob is not available in runtime',\n        respHeaders: null,\n        stage,\n        raw: { kind: 'multipart_unavailable' },\n      }),\n    };\n  }\n\n  const fd = new FormData();\n  const blob = new Blob([buffer], {\n    type: contentType || 'application/octet-stream',\n  });\n  fd.append('data', blob, 'file');\n\n  let resp;\n  try {\n    resp = await fetchWithTimeout(\n      url,\n      {\n        method: 'POST',\n        body: fd,\n      },\n      timeoutMs,\n    );\n  } catch (e) {\n    const message = e?.name === 'AbortError' ? 'timeout' : String(e?.message ?? e);\n    return {\n      ok: false,\n      status: 0,\n      error: classifyError({\n        status: 0,\n        body: null,\n        message,\n        respHeaders: null,\n        stage,\n        raw: { kind: 'upload_fetch_error' },\n      }),\n    };\n  }\n\n  if (!resp.ok) {\n    const textValue = await resp.text().catch(() => '');\n    const parsedWrap = safeJsonParse(textValue);\n    const parsed = parsedWrap.value;\n    const messageWrap = extractMessage(parsed ?? textValue, `http_${resp.status}`);\n    const message = messageWrap.value;\n\n    return {\n      ok: false,\n      status: resp.status,\n      error: classifyError({\n        status: resp.status,\n        body: parsed ?? textValue,\n        message,\n        respHeaders: resp.headers,\n        stage,\n        raw: null,\n      }),\n    };\n  }\n\n  return { ok: true, status: resp.status, error: null };\n};\n\nconst sendMessage = async ({ attachments }) => {\n  const payload = {};\n  if (text.length > 0) payload.text = text;\n  if (format) payload.format = format;\n  if (Array.isArray(attachments) && attachments.length > 0) {\n    payload.attachments = attachments;\n  }\n\n  return await requestJson({\n    url:\n      baseUrl +\n      '/messages?user_id=' +\n      encodeURIComponent(String(targetId)),\n    method: 'POST',\n    body: payload,\n    timeoutMs: timeoutMsMessage,\n    stage: 'send_message',\n  });\n};\n\nconst pickUploadFields = (body) => {\n  const uploadId = body?.upload_id ?? body?.id ?? body?.data?.id ?? null;\n  const uploadUrl =\n    body?.upload_url ??\n    body?.url ??\n    body?.data?.upload_url ??\n    body?.data?.url ??\n    null;\n  const expiresAt =\n    body?.expires_at ??\n    body?.expiresAt ??\n    body?.data?.expires_at ??\n    body?.data?.expiresAt ??\n    null;\n  return { uploadId, uploadUrl, expiresAt };\n};\n\nconst seriesProgressDelta = { acked_indexes: [] };\nconst seriesResults = [];\n\nif (allMediaForValidation.length > 0 && mediaQueue.length === 0) {\n  const progressMessageIds = getProgressProviderMessageIds(incomingSeriesProgress);\n  const providerMessageId =\n    progressMessageIds[progressMessageIds.length - 1] ?? `max-${Date.now()}`;\n\n  return wrapOk({\n    ok: true,\n    provider_message_id: String(providerMessageId),\n    series_progress_delta: seriesProgressDelta,\n    raw: {\n      series_results: seriesResults,\n      skipped_acked_only_retry: true,\n      already_acked_indexes: alreadyAckedIndexes,\n    },\n  });\n}\n\nif (allMediaForValidation.length === 0) {\n  const msg = await sendMessage({ attachments: [] });\n  if (!msg.ok) {\n    return wrapErr({\n      ok: false,\n      error: msg.error,\n      series_progress_delta: seriesProgressDelta,\n      raw: { series_results: seriesResults },\n    });\n  }\n\n  const providerMessageId =\n    msg.body?.message_id ?? msg.body?.id ?? msg.body?.data?.id ?? `max-${Date.now()}`;\n\n  return wrapOk({\n    ok: true,\n    provider_message_id: String(providerMessageId),\n    series_progress_delta: seriesProgressDelta,\n    raw: {\n      series_results: seriesResults,\n      text_only: true,\n      message_result: msg.body ?? null,\n    },\n  });\n}\n\nfor (const entry of mediaQueue) {\n  const item = entry.item;\n  const originalIndex = entry.original_index;\n\n  if (!item || typeof item !== 'object') continue;\n\n  const kindRaw = String(item.type ?? item.kind ?? '').toLowerCase();\n  const uploadType = kindRaw.includes('video') ? 'video' : 'image';\n\n  const existingToken =\n    item.upload_id ??\n    item.upload_token ??\n    item.max_upload_id ??\n    item.token ??\n    (item.attachment_id ? null : item.file_id) ??\n    null;\n\n  let uploadId = existingToken ? String(existingToken) : null;\n\n  if (!uploadId) {\n    const originUrl = item.origin_url ?? item.url ?? item.media_url ?? null;\n    if (!originUrl) {\n      return wrapErr({\n        ok: false,\n        error: {\n          category: 'PERMANENT',\n          scope: 'delivery',\n          code: 'media_missing_origin',\n          message: `media[${originalIndex}] missing origin_url/url/media_url`,\n          raw: { index: originalIndex, item },\n        },\n        series_progress_delta: seriesProgressDelta,\n        raw: { series_results: seriesResults },\n      });\n    }\n\n    const create = await requestJson({\n      url: `${baseUrl}/uploads?type=${encodeURIComponent(uploadType)}`,\n      method: 'POST',\n      body: {},\n      timeoutMs: timeoutMsUpload,\n      stage: 'create_upload',\n    });\n    if (!create.ok) {\n      return wrapErr({\n        ok: false,\n        error: create.error,\n        series_progress_delta: seriesProgressDelta,\n        raw: { series_results: seriesResults },\n      });\n    }\n\n    const { uploadId: createdId, uploadUrl, expiresAt } = pickUploadFields(create.body);\n    if (!createdId || !uploadUrl) {\n      return wrapErr({\n        ok: false,\n        error: {\n          category: 'TRANSIENT',\n          scope: 'platform',\n          code: 'upload_session_unexpected',\n          message: 'MAX upload session response missing upload_id/upload_url',\n          raw: { body: create.body },\n        },\n        series_progress_delta: seriesProgressDelta,\n        raw: { series_results: seriesResults },\n      });\n    }\n\n    const downloaded = await requestBinary({\n      url: String(originUrl),\n      timeoutMs: timeoutMsDownload,\n      stage: 'download_origin',\n    });\n    if (!downloaded.ok) {\n      return wrapErr({\n        ok: false,\n        error: downloaded.error,\n        series_progress_delta: seriesProgressDelta,\n        raw: { series_results: seriesResults },\n      });\n    }\n\n    const uploaded = await uploadMultipart({\n      url: String(uploadUrl),\n      buffer: downloaded.buffer,\n      contentType: downloaded.contentType,\n      timeoutMs: timeoutMsUpload,\n      stage: 'upload_binary',\n    });\n    if (!uploaded.ok) {\n      return wrapErr({\n        ok: false,\n        error: uploaded.error,\n        series_progress_delta: seriesProgressDelta,\n        raw: { series_results: seriesResults },\n      });\n    }\n\n    uploadId = String(createdId);\n    appendCacheUpdate({\n      blobRef: item.blob_ref ?? null,\n      fileId: uploadId,\n      mediaIndex: originalIndex,\n      source: 'upload_binary',\n      expiresAt,\n    });\n  }\n\n  const msg = await sendMessage({ attachments: [{ upload_id: uploadId }] });\n  if (!msg.ok) {\n    return wrapErr({\n      ok: false,\n      error: msg.error,\n      series_progress_delta: seriesProgressDelta,\n      raw: { series_results: seriesResults },\n    });\n  }\n\n  const providerMessageId =\n    msg.body?.message_id ?? msg.body?.id ?? msg.body?.data?.id ?? `max-${Date.now()}`;\n\n  seriesProgressDelta.acked_indexes.push(originalIndex);\n  seriesResults.push({\n    index: originalIndex,\n    upload_id: uploadId,\n    provider_message_id: String(providerMessageId),\n    raw: msg.body,\n  });\n}\n\nconst primaryProviderMessageId =\n  seriesResults[0]?.provider_message_id ??\n  getProgressProviderMessageIds(incomingSeriesProgress).slice(-1)[0] ??\n  `max-${Date.now()}`;\n\nreturn wrapOk({\n  ok: true,\n  provider_message_id: String(primaryProviderMessageId),\n  series_progress_delta: seriesProgressDelta,\n  raw: { series_results: seriesResults },\n});"
      },
      "id": "3b495dcf-682c-4314-8f8d-de179c7279f0",
      "name": "MAX Send (Code)",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1792,
        0
      ]
    },
    {
      "parameters": {
        "jsCode": "const input = $input.first()?.json ?? {};\n\nconst safeParseJson = (value) => {\n  if (value && typeof value === 'object') return value;\n  if (typeof value !== 'string') return null;\n  try {\n    return JSON.parse(value);\n  } catch {\n    return null;\n  }\n};\n\nconst stripInternal = (value) => {\n  if (!value || typeof value !== 'object' || Array.isArray(value)) return value;\n  const cloned = { ...value };\n  delete cloned.cache_updates;\n  return cloned;\n};\n\nconst resultFromDb = safeParseJson(input.result_json);\nif (resultFromDb && typeof resultFromDb === 'object') {\n  return [{ json: stripInternal(resultFromDb) }];\n}\n\nif (input?.result && typeof input.result === 'object') {\n  return [{ json: stripInternal(input.result) }];\n}\n\nif (input?.ok === true || input?.ok === false) {\n  return [{ json: stripInternal(input) }];\n}\n\nreturn [\n  {\n    json: {\n      ok: false,\n      error: {\n        category: 'TRANSIENT',\n        scope: 'platform',\n        code: 'adapter_output_unexpected',\n        message: 'adapter_max_send returned unexpected payload shape',\n        raw: input,\n      },\n    },\n  },\n];"
      },
      "id": "930fb336-aeb5-4728-a591-5cf12bae0367",
      "name": "Finalize Adapter Output",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        2000,
        240
      ]
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "WITH req AS (\n  SELECT\n    $1::text AS workspace_id,\n    COALESCE($2::jsonb, '[]'::jsonb) AS refs_json\n),\nrefs AS (\n  SELECT DISTINCT value::text AS blob_ref\n  FROM req, jsonb_array_elements_text(req.refs_json)\n  WHERE value IS NOT NULL\n    AND value <> ''\n),\nresolved AS (\n  SELECT\n    refs.blob_ref,\n    mb.file_id\n  FROM refs\n  LEFT JOIN public.media_blobs mb\n    ON mb.workspace_id::text = (SELECT workspace_id FROM req)\n   AND mb.blob_ref = refs.blob_ref\n   AND mb.provider = 'max'\n)\nSELECT blob_ref, file_id\nFROM resolved\nUNION ALL\nSELECT NULL::text AS blob_ref, NULL::text AS file_id\nWHERE NOT EXISTS (SELECT 1 FROM resolved);",
        "options": {
          "queryReplacement": "={{$json.workspace_id}}, {{ JSON.stringify($json.media_blob_refs || []) }}"
        }
      },
      "id": "3084897b-0308-436a-81db-5e99588b8ee6",
      "name": "DB Resolve MAX Media Cache",
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.6,
      "position": [
        1408,
        0
      ],
      "credentials": {
        "postgres": {
          "id": "x5LyxEPUED2WoWLF",
          "name": "Neon Autoposting DB"
        }
      },
      "onError": "continueRegularOutput"
    },
    {
      "parameters": {
        "jsCode": "const base = $items('Normalize Adapter Input')[0]?.json ?? {};\nconst rows = $input.all().map((item) => item?.json ?? {});\n\nconst cacheByBlobRef = {};\nfor (const row of rows) {\n  const blobRef = typeof row?.blob_ref === 'string' ? row.blob_ref.trim() : '';\n  const fileId = typeof row?.file_id === 'string' ? row.file_id.trim() : '';\n  if (!blobRef || !fileId) continue;\n  cacheByBlobRef[blobRef] = fileId;\n}\n\nconst media = Array.isArray(base.media)\n  ? base.media.map((item) => {\n      if (!item || typeof item !== 'object') return item;\n\n      const blobRef = typeof item.blob_ref === 'string' ? item.blob_ref.trim() : '';\n      const cachedUploadToken = blobRef ? cacheByBlobRef[blobRef] ?? null : null;\n\n      const existingUploadToken =\n        item.upload_id ?? item.upload_token ?? item.max_upload_id ?? item.token ?? null;\n      const resolvedUploadToken = existingUploadToken ?? cachedUploadToken;\n\n      return {\n        ...item,\n        upload_id: resolvedUploadToken ?? item.upload_id ?? null,\n        upload_token: resolvedUploadToken ?? item.upload_token ?? null,\n        max_upload_id: resolvedUploadToken ?? item.max_upload_id ?? null,\n        token: item.token ?? resolvedUploadToken ?? null,\n        attachment_id: item.attachment_id ?? null,\n        file_id: item.file_id ?? null,\n      };\n    })\n  : [];\n\nreturn [\n  {\n    json: {\n      ...base,\n      media,\n    },\n  },\n];"
      },
      "id": "72c0a960-3ab6-43c7-9c84-d94f9d409ec9",
      "name": "Apply MAX Media Cache",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1600,
        0
      ]
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "version": 3,
            "leftValue": "",
            "caseSensitive": true,
            "typeValidation": "strict"
          },
          "conditions": [
            {
              "id": "max-cond-has-cache-updates",
              "leftValue": "={{ !!$json.workspace_id && Array.isArray($json.cache_updates) && $json.cache_updates.length > 0 }}",
              "rightValue": "",
              "operator": {
                "type": "boolean",
                "operation": "true",
                "singleValue": true
              }
            }
          ],
          "combinator": "and"
        },
        "options": {}
      },
      "id": "max-if-has-cache-updates",
      "name": "Has MAX Cache Updates?",
      "type": "n8n-nodes-base.if",
      "typeVersion": 2.3,
      "position": [
        1888,
        96
      ]
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "WITH payload AS (\n  SELECT\n    COALESCE($1::jsonb, '[]'::jsonb) AS updates_json,\n    COALESCE($2::jsonb, '{}'::jsonb) AS result_json\n),\nupdates AS (\n  SELECT\n    NULLIF(value->>'workspace_id', '') AS workspace_id,\n    NULLIF(value->>'blob_ref', '') AS blob_ref,\n    NULLIF(value->>'file_id', '') AS file_id,\n    NULLIF(value->>'expires_at', '')::timestamptz AS expires_at,\n    COALESCE(value->'meta', '{}'::jsonb) AS meta\n  FROM payload, jsonb_array_elements(payload.updates_json) AS value\n),\nvalid_updates AS (\n  SELECT *\n  FROM updates\n  WHERE workspace_id IS NOT NULL\n    AND blob_ref IS NOT NULL\n    AND file_id IS NOT NULL\n),\nupserted AS (\n  INSERT INTO public.media_blobs (\n    workspace_id,\n    blob_ref,\n    provider,\n    file_id,\n    expires_at,\n    meta,\n    created_at,\n    updated_at\n  )\n  SELECT\n    vu.workspace_id::uuid,\n    vu.blob_ref,\n    'max',\n    vu.file_id,\n    vu.expires_at,\n    vu.meta,\n    now(),\n    now()\n  FROM valid_updates vu\n  INNER JOIN public.media_origin mo\n    ON mo.workspace_id = vu.workspace_id::uuid\n   AND mo.blob_ref = vu.blob_ref\n  ON CONFLICT (workspace_id, blob_ref, provider)\n  DO UPDATE SET\n    file_id = EXCLUDED.file_id,\n    expires_at = COALESCE(EXCLUDED.expires_at, public.media_blobs.expires_at),\n    meta = COALESCE(public.media_blobs.meta, '{}'::jsonb) || COALESCE(EXCLUDED.meta, '{}'::jsonb),\n    updated_at = now()\n  RETURNING file_id\n)\nSELECT payload.result_json - 'cache_updates' AS result_json\nFROM payload;",
        "options": {
          "queryReplacement": "={{ JSON.stringify($json.cache_updates || []) }}, {{ JSON.stringify($json) }}"
        }
      },
      "id": "max-db-upsert-media-cache",
      "name": "DB Upsert MAX Media Cache",
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.6,
      "position": [
        1888,
        -64
      ],
      "credentials": {
        "postgres": {
          "id": "x5LyxEPUED2WoWLF",
          "name": "Neon Autoposting DB"
        }
      },
      "onError": "continueRegularOutput"
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "WITH req AS (\n  SELECT\n    NULLIF($1::text, '')::uuid AS workspace_id,\n    NULLIF($2::text, '') AS channel_id,\n    NULLIF($3::text, '') AS auth_ref\n),\ncandidate AS (\n  SELECT\n    c.auth_ref,\n    c.settings\n  FROM public.channels c\n  JOIN req r\n    ON r.workspace_id = c.workspace_id\n  WHERE c.platform = 'max'\n    AND (\n      (r.channel_id IS NOT NULL AND c.channel_id = r.channel_id)\n      OR (r.channel_id IS NULL AND r.auth_ref IS NOT NULL AND c.auth_ref = r.auth_ref)\n    )\n  ORDER BY\n    CASE\n      WHEN (SELECT channel_id FROM req) IS NOT NULL\n       AND c.channel_id = (SELECT channel_id FROM req) THEN 0\n      ELSE 1\n    END,\n    c.channel_id\n  LIMIT 1\n)\nSELECT\n  COALESCE(candidate.settings->>'max_auth', candidate.settings->>'auth_token', candidate.settings->>'token') AS auth_value,\n  COALESCE(candidate.settings->>'auth_header_name', 'Authorization') AS auth_header_name,\n  COALESCE(candidate.settings->>'max_base_url', candidate.settings->>'base_url') AS max_base_url,\n  candidate.auth_ref AS resolved_auth_ref\nFROM candidate\nUNION ALL\nSELECT\n  NULL::text AS auth_value,\n  NULL::text AS auth_header_name,\n  NULL::text AS max_base_url,\n  NULL::text AS resolved_auth_ref\nWHERE NOT EXISTS (SELECT 1 FROM candidate);",
        "options": {
          "queryReplacement": "={{$json.workspace_id}}, {{$json.channel_id || $json.delivery?.channel_id || ''}}, {{$json.auth_ref || $json.channel?.auth_ref || ''}}"
        }
      },
      "id": "max-db-resolve-auth-9b2e1d9a",
      "name": "DB Resolve MAX Auth",
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.6,
      "position": [
        1168,
        72
      ],
      "credentials": {
        "postgres": {
          "id": "x5LyxEPUED2WoWLF",
          "name": "Neon Autoposting DB"
        }
      },
      "onError": "continueRegularOutput"
    },
    {
      "parameters": {
        "jsCode": "const base = $items('Normalize Adapter Input')[0]?.json ?? {};\nconst row = $input.first()?.json ?? {};\n\nconst cleanString = (value) => {\n  if (value === null || value === undefined) return '';\n  return String(value).trim();\n};\n\nconst directAuth = cleanString(base.auth_header_value);\nconst resolvedAuth = cleanString(row.auth_value);\nconst mergedAuth = directAuth || resolvedAuth;\n\nconst directHeaderName = cleanString(base.auth_header_name);\nconst resolvedHeaderName = cleanString(row.auth_header_name);\nconst mergedHeaderName = directHeaderName || resolvedHeaderName || 'Authorization';\n\nlet mergedBaseUrl = cleanString(base.max_base_url) || cleanString(row.max_base_url);\nif (!mergedBaseUrl) mergedBaseUrl = 'https://platform-api.max.ru';\nwhile (mergedBaseUrl.endsWith('/')) mergedBaseUrl = mergedBaseUrl.slice(0, -1);\n\nconst mergedAuthRef = cleanString(base.auth_ref) || cleanString(row.resolved_auth_ref);\n\nreturn [\n  {\n    json: {\n      ...base,\n      auth_ref: mergedAuthRef || null,\n      auth_header_name: mergedHeaderName,\n      auth_header_value: mergedAuth,\n      max_base_url: mergedBaseUrl,\n    },\n  },\n];"
      },
      "id": "max-apply-auth-5c2f4a1c",
      "name": "Apply MAX Auth",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1360,
        240
      ]
    }
  ],
  "pinData": {},
  "connections": {
    "When Executed by Another Workflow": {
      "main": [
        [
          {
            "node": "Normalize Adapter Input",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Adapter MAX Smoke Webhook": {
      "main": [
        [
          {
            "node": "Normalize Adapter Input",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Normalize Adapter Input": {
      "main": [
        [
          {
            "node": "DB Resolve MAX Auth",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Can Call MAX API?": {
      "main": [
        [
          {
            "node": "DB Resolve MAX Media Cache",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Simulate MAX Success",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Simulate MAX Success": {
      "main": [
        [
          {
            "node": "Finalize Adapter Output",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "MAX Send (Code)": {
      "main": [
        [
          {
            "node": "Has MAX Cache Updates?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "DB Resolve MAX Media Cache": {
      "main": [
        [
          {
            "node": "Apply MAX Media Cache",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Apply MAX Media Cache": {
      "main": [
        [
          {
            "node": "MAX Send (Code)",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Has MAX Cache Updates?": {
      "main": [
        [
          {
            "node": "DB Upsert MAX Media Cache",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Finalize Adapter Output",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "DB Upsert MAX Media Cache": {
      "main": [
        [
          {
            "node": "Finalize Adapter Output",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "DB Resolve MAX Auth": {
      "main": [
        [
          {
            "node": "Apply MAX Auth",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Apply MAX Auth": {
      "main": [
        [
          {
            "node": "Can Call MAX API?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "active": true,
  "settings": {
    "executionOrder": "v1",
    "callerPolicy": "workflowsFromSameOwner",
    "availableInMCP": false
  },
  "versionId": "2cf618b8-40e3-4f82-a24b-2767e1e8e620",
  "meta": {
    "instanceId": "ac845f023808ccf31cb6cf4b5e79d811bd2246113c33a345a99a14afb7f8967b"
  },
  "id": "Vmk0MqDaJDFos5sM",
  "tags": []
}
